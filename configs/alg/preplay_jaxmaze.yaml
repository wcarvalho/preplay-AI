ALG: 'dynaq'

# EXPERIMENT PARAMS
NUM_SEEDS: 1
SEED: 1

# logggin
MAX_EPISODE_LOG_LEN: 40


# RUN PARAMS
TOTAL_TIMESTEPS: 30_000_000
NUM_ENVS: 32
EVAL_STEPS: 100
EVAL_EPISODES: 100
LEARNER_LOG_PERIOD: 500
LEARNER_EXTRA_LOG_PERIOD: 5_00
#LEARNER_EXTRA_LOG_PERIOD: 0
EVAL_LOG_PERIOD: 25
EVAL_LOG_PERIOD_ACTOR: 0
#GRADIENT_LOG_PERIOD: 5_000
GRADIENT_LOG_PERIOD: 0

# NEURAL NET PARAMS
EMBED_HIDDEN_DIM: 64
NUM_MLP_LAYERS: 0
NUM_EMBED_LAYERS: 0
MLP_HIDDEN_DIM: 256
AGENT_RNN_DIM: 256
ACTIVATION: 'leaky_relu'
NUM_Q_LAYERS: 2
NUM_PRED_LAYERS: 2
Q_HIDDEN_DIM: 1024  # DIFFERENT FROM QLEARNING
QHEAD_TYPE: 'duelling'
OBS_INCLUDE_GOAL: True
SHARE_Q_FN: True
USE_BIAS: False

# ALGO PARAMS
BUFFER_SIZE: 50_000
TOTAL_BATCH_SIZE: 1280
BUFFER_BATCH_SIZE: 32
SAMPLE_LENGTH: null
LEARNING_STARTS: 10_000
TRAINING_INTERVAL: 5

# ONTASK DYNA PARAMS
ONTASK_LOSS_COEFF: 1.0
TD_LAMBDA: 0.9
ALL_GOALS_LAMBDA: 0.6

# OFFTASK DYNA PARAMS
OFFTASK_LOSS_COEFF: 1.0
ALL_GOALS_COEFF: 1.0
KNOWN_OFFTASK_GOAL: False
NUM_ONTASK_SIMULATIONS: 1
NUM_OFFTASK_SIMULATIONS: 1
NUM_OFFTASK_GOALS: 1
MAINQ_COEFF: 1e-2
OFFTASK_COEFF: 2.0
SUBTASK_COEFF: 2.0  # deprecated, use OFFTASK_COEFF

# General Preplay Params
ONLINE_COEFF: 1.0  # online loss
DYNA_COEFF: 1.0  # simulation loss
SIMULATION_LENGTH: 15
SIM_EPSILON_SETTING: 1 # range of epsilons
WINDOW_SIZE: 1.0
STEP_COST: 0.0

# Online epsilon settings
FIXED_EPSILON: 1 # range of epsilons
EPSILON_START: 1.0
EPSILON_FINISH: 0.1
EPSILON_ANNEAL_TIME: 5e5

# Training Params
MAX_GRAD_NORM: 80
TARGET_UPDATE_INTERVAL: 1_000
LR: 0.0003
LR_LINEAR_DECAY: FALSE
EPS_ADAM: 0.00001

GAMMA: 0.992
